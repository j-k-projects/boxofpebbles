{
  
    
        "post0": {
            "title": "Principal Component Analysis (PCA) from Scratch",
            "content": ". Relevance . Principal Component Analysis (PCA) is a data-reduction technique that finds application in a wide variety of fields, including biology, sociology, physics, medicine, and audio processing. PCA may be used as a &quot;front end&quot; processing step that feeds into additional layers of machine learning, or it may be used by itself, for example when doing data visualization. It is so useful and ubiquitious that is is worth learning not only what it is for and what it is, but how to actually do it. . In this interactive worksheet, we work through how to perform PCA on a few different datasets, writing our own code as we go. . Other (better?) treatments . My treatment here was written several months after viewing... . the excellent demo page at setosa.io | this quick 1m30s video of a teapot, | this great StatsQuest video | this lecture from Andrew Ng&#39;s course | . Basic Idea . Put simply, PCA involves making a coordinate transformation (i.e., a rotation) from the arbitrary axes (or &quot;features&quot;) you started with to a set of axes &#39;aligned with the data itself,&#39; and doing this almost always means that you can get rid of a few of these &#39;components&#39; of data that have small variance without suffering much in the way of accurcy while saving yourself a ton of computation. . Once you &quot;get it,&quot; you&#39;ll find PCA to be almost no big deal, if it weren&#39;t for the fact that it&#39;s so darn useful! . We&#39;ll define the following terms as we go, but here&#39;s the process in a nutshell: . Covariance: Find the covariance matrix for your dataset | Eigenvectors: Find the eigenvectors of that matrix (these are the &quot;components&quot; btw) | Ordering: Sort the eigenvectors/&#39;dimensions&#39; from biggest to smallest variance | Projection / Data reduction: Use the eigenvectors corresponding to the largest variance to project the dataset into a reduced- dimensional space | (Check: How much did we lose by that truncation?) | Caveats . Since PCA will involve making linear transformations, there are some situations where PCA won&#39;t help but...pretty much it&#39;s handy enough that it&#39;s worth giving it a shot! . Covariance . If you&#39;ve got two data dimensions and they vary together, then they are co-variant. . Example: Two-dimensional data that&#39;s somewhat co-linear: . import numpy as np import matplotlib.pyplot as plt import plotly.graph_objects as go N = 100 x = np.random.normal(size=N) y = 0.5*x + 0.2*(np.random.normal(size=N)) fig = go.Figure(data=[go.Scatter(x=x, y=y, mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; )]) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . . . Variance . So, for just the $x$ component of the above data, there&#39;s some mean value (which in this case is zero), and there&#39;s some variance about this mean: technically the variance is the average of the squared differences from the mean. If you&#39;re familiar with the standard deviation, usually denoted by $ sigma$, the variance is just the square of the standard deviation. If $x$ had units of meters, the variance $ sigma^2$ would have units of meters^2. . Think of the variance as the &quot;spread,&quot; or &quot;extent&quot; of the data, about some particular axis (or input, or &quot;feature&quot;). . Similarly we can look at the variance in just the $y$ component of the data. For the above data, the variances in $x$ and $y$ are . print(&quot;Variance in x =&quot;,np.var(x)) print(&quot;Variance in y =&quot;,np.var(y)) . Variance in x = 0.6470431671825421 Variance in y = 0.19318628312072175 . Covariance . You&#39;ll notice in the above graph that as $x$ varies, so does $y$ -- pretty much. So $y$ is &quot;covariant&quot; with $x$. &quot;Covariance indicates the level to which two variables vary together.&quot; To compute it, it&#39;s kind of like the regular variance, except that instead of squaring the deviation from the mean for one variable, we multiply the deviations for the two variables: . $${ rm Cov}(x,y) = {1 over N-1} sum_{j=1}^N (x_j- mu_x)(y_j- mu_j),$$ where $ mu_x$ and $ mu_y$ are the means for the x- and y- componenets of the data, respectively. Note that you can reverse $x$ and $y$ and get the same result, and the covariance of a variable with itself is just the regular variance -- but with one caveat! . The caveat is that we&#39;re dividing by $N-1$ instead of $N$, so unlike the regular variance we&#39;re not quite taking the mean. Why this? Well, for large datasets this makes essentially no difference, but for small numbers of data points, using $N$ can give values that tend to be a bit too small for most people&#39;s tastes, so the $N-1$ was introduced to &quot;reduce small sample bias.&quot; . In Python code, the covariance calculation looks like . def covariance(a,b): return ( (a - a.mean())*(b - b.mean()) ).sum() / (len(a)-1) print(&quot;Covariance of x &amp; y =&quot;,covariance(x,y)) print(&quot;Covariance of y &amp; x =&quot;,covariance(x,y)) print(&quot;Covariance of x with itself =&quot;,covariance(x,x),&quot;, variance of x =&quot;,np.var(x)) print(&quot;Covariance of y with itself =&quot;,covariance(y,y),&quot;, variance of x =&quot;,np.var(y)) . Covariance of x &amp; y = 0.3211758726837525 Covariance of y &amp; x = 0.3211758726837525 Covariance of x with itself = 0.6535789567500425 , variance of x = 0.6470431671825421 Covariance of y with itself = 0.19513765971790076 , variance of x = 0.19318628312072175 . Covariance matrix . So what we do is we take the covariance of every variable with every variable (including itself) and make a matrix out of it. Along the diagonal will be the variance of each variable (except for that $N-1$ in the denominator), and the rest of the matrix will be the covariances. Note that since the order of the variables doesn&#39;t matter when computing covariance, the matrix will be symmetric (i.e. it will equal its own transpose, i.e. will have a reflection symmetry across the diagonal) and thus will be a square matrix. . Numpy gives us a handy thing to call: . data = np.stack((x,y),axis=1) # pack the x &amp; y data together in one 2D array print(&quot;data.shape =&quot;,data.shape) cov = np.cov(data.T) # .T b/c numpy wants varibles along rows rather than down columns? print(&quot;covariance matrix = n&quot;,cov) . data.shape = (100, 2) covariance matrix = [[0.65357896 0.32117587] [0.32117587 0.19513766]] . Some 3D data to work with . Now that we know what a covariance matrix is, let&#39;s generate some 3D data that we can use for what&#39;s coming next. Since there are 3 variables or 3 dimensions, the covariance matrix will now be 3x3. . z = -.5*x + 2*np.random.uniform(size=N) data = np.stack((x,y,z)).T print(&quot;data.shape =&quot;,data.shape) cov = np.cov(data.T) print(&quot;covariance matrix = n&quot;,cov) # Plot our data import plotly.graph_objects as go fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; )]) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . data.shape = (100, 3) covariance matrix = [[ 0.65357896 0.32117587 -0.33982198] [ 0.32117587 0.19513766 -0.15839307] [-0.33982198 -0.15839307 0.5207214 ]] . . . (Note that even though our $z$ data didn&#39;t explicitly depend on $y$, the fact that $y$ is covariant with $x$ means that $y$ and $z$ &#39;coincidentally&#39; have a nonzero covariance. This sort of thing shows up in many datasets where two variables are correlated and may give rise to &#39;confounding&#39; factors.) . So now we have a covariance matrix. The next thing in PCA is find the &#39;principal components&#39;. This means the directions along which the data varies the most. You can kind of estimate these by rotating the 3D graph above. See also this great YouTube video of a teapot (1min 30s) that explains PCA in this manner. . To do Principal Component Analysis, we need to find the aforementioned &quot;components,&quot; and this requires finding eigenvectors for our dataset&#39;s covariance matrix. . What is an eigenvector, really? . First a definition. (Stay with me! We&#39;ll flesh this out in what comes after this.) . Given some matrix (or &#39;linear operator&#39;) ${ bf A}$ with dimensions $n times n$ (i.e., $n$ rows and $n$ columns), there exist a set of $n$ vectors $ vec{v}_i$ (each with dimension $n$, and $i = 1...n$ counts which vector we&#39;re talking about) such that multiplying one of these vectors by ${ bf A}$ results in a vector (anti)parallel to $ vec{v}_i$, with a length that&#39;s multiplied by some constant $ lambda_i$. In equation form: . $${ bf A} vec{v}_i = lambda_i vec{v}_i, (1)$$ . where the constants $ lambda_i$ are called eigenvalues and the vectors $ vec{v}_i$ are called eigenvectors. . (Note that I&#39;m departing a bit from common notation that uses $ vec{x}_i$ instead of $ vec{v}_i$; I don&#39;t want people to get confused when I want to use $x$&#39;s for coordinate variables.) . A graphical version of this is shown in Figure 1: . . Figure 1. &quot;An eigenvector is a vector that a linear operator sends to a multiple of itself&quot; -- Daniel McLaury . Brief Q &amp; A before we go on: . &quot;So what&#39;s with the &#39;eigen&#39; part?&quot; That&#39;s a German prefix, which in this context means &quot;inherent&quot; or &quot;own&quot;. | &quot;Can a non-square matrix have eigenvectors?&quot; Well,...no, and think of it this way: If $ bf{A}$ were an $n times m$ matrix (where $m &lt;&gt; n$), then it would be mapping from $n$ dimensions into $m$ dimensions, but on the &quot;other side&quot; of the equation with the $ lambda_i vec{v}_i$, that would still have $n$ dimensions, so... you&#39;d be saying an $n$-dimensional object equals an $m$-dimensional object, which is a no-go. | &quot;But my dataset has many more rows than columns, so what am I supposed to do about that?&quot; Just wait! It&#39;ll be ok. We&#39;re not actually going to take the eigenvectors of the dataset &#39;directly&#39;, we&#39;re going to take the eigenvectors of the covariance matrix of the dataset. | &quot;Are eigenvectors important?&quot; You bet! They get used in many areas of science. I first encountered them in quantum mechanics.* They describe the &quot;principal vectors&quot; of many objects, or &quot;normal modes&quot; of oscillating systems. They get used in computer vision, and... lots of places. You&#39;ll see them almost anywhere matrices &amp; tensors are employed, such as our topic for today: Data science! | *Ok that&#39;s not quite true: I first encountered them in an extremely boring Linear Algebra class taught by an extremely boring NASA engineer who thought he wanted to try teaching. But it wasn&#39;t until later that I learned anything about their relevance for...anything. Consquently I didn&#39;t &quot;learn them&quot; very well so writing this is a helpful review for me. . How to find the eigenvectors of a matrix . You call a library routine that does it for you, of course! ;-) . from numpy import linalg as LA lambdas, vs = LA.eig(cov) lambdas, vs . (array([1.07311435, 0.26724004, 0.02908363]), array([[-0.73933506, -0.47534042, -0.47690162], [-0.3717427 , -0.30238807, 0.87770657], [ 0.56141877, -0.82620393, -0.04686177]])) . Ok sort of kidding; we&#39;ll do it &quot;from scratch&quot;. But, one caveat before we start: Some matrices can be &quot;weird&quot; or &quot;problematic&quot; and have things like &quot;singular values.&quot; There are sophisticated numerical libraries for doing this, and joking aside, for real-world numerical applications you&#39;re better off calling a library routine that other very smart and very careful people have written for you. But for now, we&#39;ll do the straightforward way which works pretty well for many cases. We&#39;ll follow the basic two steps: . Find the eigenvalues | &#39;Plug in&#39; each eigenvalue to get a system of linear equations for the values of the components of the corresponding eigenvector | Solve this linear system. | 1. Find the eigenvalues . Ok I&#39;m hoping you at least can recall what a determinant of a matrix is. Many people, even if they don&#39;t know what a determinant is good for (e.g. tons of proofs &amp; properties all rest on the determinant), they still at least remember how to calculate one. . The way to get the eigenvalues is to take the determinant of the difference between a $ bf{A}$ and $ lambda$ times the identity matrix $ bf{I}$ (which is just ones along the diagonal and zeros otherwise) and set that difference equal to zero... . $$det( bf{A} - lambda I) = 0 $$ . Just another observation:Since ${ bf I}$ is a square matrix, that means $ bf{A}$ has to be a square matrix too. Then solving for $ lambda$ will give you a polynomial equation in $ lambda$, the solutions to (or roots of) which are the eigenvectors $ lambda_i$. . Let&#39;s do an example: . $$ { bf A} = begin{bmatrix} -2 &amp; 2 &amp; 1 -5 &amp; 5 &amp; 1 -4 &amp; 2 &amp; 3 end{bmatrix} $$To find the eigenvalues we set $$ det( bf{A} - lambda I) = begin{vmatrix} -2- lambda &amp; 2 &amp; 1 -5 &amp; 5- lambda &amp; 1 -4 &amp; 2 &amp; 3- lambda end{vmatrix} = 0.$$ . This gives us the equation... $$0 = lambda^3 - 6 lambda^2 + lambda - 6$$ . which has the 3 solutions (in descending order) $$ lambda = 3, 2, 1.$$ . *(Aside: to create an integer matrix with integer eigenvalues, I used [this handy web tool](https://ericthewry.github.io/integer_matrices/))*. . Just to check that against the numpy library: . A = np.array([[-2,2,1],[-5,5,1],[-4,2,3]]) def sorted_eig(A): # For now we sort &#39;by convention&#39;. For PCA the sorting is key. lambdas, vs = LA.eig(A) # Next line just sorts values &amp; vectors together in order of decreasing eigenvalues lambdas, vs = zip(*sorted(zip(list(lambdas), list(vs.T)),key=lambda x: x[0], reverse=True)) return lambdas, np.array(vs).T # un-doing the list-casting from the previous line lambdas, vs = sorted_eig(A) lambdas # hold off on printing out the eigenvectors until we do the next part! . (3.0000000000000027, 1.9999999999999991, 1.0000000000000013) . Close enough! . 2. Use the eigenvalues to get the eigenvectors . Although it was anncounced in mid 2019 that you can get eigenvectors directly from eigenvalues, the usual way people have done this for a very long time is to go back to the matrix $ bf{A}$ and solve the linear system of equation (1) above, for each of the eigenvalues. For example, for $ lambda_1=-1$, we have . $$ { bf}A vec{v}_1 = - vec{v}_1 $$i.e. . $$ begin{bmatrix} -2 &amp; 2 &amp; 1 -5 &amp; 5 &amp; 1 -4 &amp; 2 &amp; 3 end{bmatrix} begin{bmatrix} v_{1x} v_{1y} v_{1z} end{bmatrix} = - begin{bmatrix} v_{1x} v_{1y} v_{1z} end{bmatrix} $$This amounts to 3 equations for 3 unknowns,...which I&#39;m going to assume you can handle... For the other eigenvalues things proceed similarly. The solutions we get for the 3 eigenvalues are: . $$ lambda_1 = 3: vec{v}_1 = (1,2,1)^T$$ . $$ lambda_2 = 2: vec{v}_2 = (1,1,2)^T$$ . $$ lambda_3 = 1: vec{v}_3 = (1,1,1)^T$$ . Since our original equation (1) allows us to scale eigenvectors by any artibrary constant, often we&#39;ll express eigenvectors as unit vectors $ hat{v}_i$. This will amount to dividing by the length of each vector, i.e. in our example multiplying by $(1/ sqrt{6},1/ sqrt{6},1/ sqrt{3})$. . In this setting . $$ lambda_1 = 3: hat{v}_1 = (1/ sqrt{6},2/ sqrt{6},1/ sqrt{6})^T$$ . $$ lambda_2 = 2: hat{v}_2 = (1/ sqrt{6},1/ sqrt{6},2/ sqrt{6})^T$$ . $$ lambda_3 = 1: hat{v}_3 = (1,1,1)^T/ sqrt{3}$$ . Checking our answers (left) with numpy&#39;s answers (right): . print(&quot; &quot;*15,&quot;Ours&quot;,&quot; &quot;*28,&quot;Numpy&quot;) print(np.array([1,2,1])/np.sqrt(6), vs[:,0]) print(np.array([1,1,2])/np.sqrt(6), vs[:,1]) print(np.array([1,1,1])/np.sqrt(3), vs[:,2]) . Ours Numpy [0.40824829 0.81649658 0.40824829] [-0.40824829 -0.81649658 -0.40824829] [0.40824829 0.40824829 0.81649658] [0.40824829 0.40824829 0.81649658] [0.57735027 0.57735027 0.57735027] [0.57735027 0.57735027 0.57735027] . The fact that the first one differs by a multiplicative factor of -1 is not an issue. Remember: eigenvectors can be multiplied by an arbitrary constant. (Kind of odd that numpy doesn&#39;t choose the positive version though!) . One more check: let&#39;s multiply our eigenvectors times A to see what we get: . print(&quot;A*v_1 / 3 = &quot;,np.matmul(A, np.array([1,2,1]).T)/3 ) # Dividing by eigenvalue print(&quot;A*v_2 / 2 = &quot;,np.matmul(A, np.array([1,1,2]).T)/2 ) # to get vector back print(&quot;A*v_3 / 1 = &quot;,np.matmul(A, np.array([1,1,1]).T) ) . A*v_1 / 3 = [1. 2. 1.] A*v_2 / 2 = [1. 1. 2.] A*v_3 / 1 = [1 1 1] . Great! Let&#39;s move on. Back to our data! . Eigenvectors for our sample 3D dataset . Recall we named our 3x3 covariance matrix &#39;cov&#39;. So now we&#39;ll compute its eigenvectors, and then re-plot our 3D data and also plot the 3 eigenvectors with it... . # Now that we know we can get the same answers as the numpy library, let&#39;s use it lambdas, vs = sorted_eig(cov) # Compute e&#39;vals and e&#39;vectors of cov matrix print(&quot;lambdas, vs = n&quot;,lambdas,&quot; n&quot;,vs) # Re-plot our data fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; ) ]) # Draw some extra &#39;lines&#39; showing eigenvector directions n_ev_balls = 50 # the lines will be made of lots of balls in a line ev_size= 3 # size of balls t = np.linspace(0,1,num=n_ev_balls) # parameterizer for drawing along vec directions for i in range(3): # do this for each eigenvector # Uncomment the next line to scale (unit) vector by size of the eigenvalue # vs[:,i] *= lambdas[i] ex, ey, ez = t*vs[0,i], t*vs[1,i], t*vs[2,i] fig.add_trace(go.Scatter3d(x=ex, y=ey, z=ez,mode=&#39;markers&#39;, marker=dict(size=ev_size,opacity=0.8), name=&quot;v_&quot;+str(i+1))) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . lambdas, vs = (1.073114351318777, 0.26724003566904386, 0.0290836286576176) [[-0.73933506 -0.47534042 -0.47690162] [-0.3717427 -0.30238807 0.87770657] [ 0.56141877 -0.82620393 -0.04686177]] . . . Things to note from the above graph: . The first (red) eigenvector points along the direction of biggest variance | The second (greenish) eigenvector points along the direction of second-biggest variance | The third (purple) eigenvector points along the direction of smallest variance. | (If you edit the above code to rescale the vector length by the eigenvector, you&#39;ll really see these three point become apparent!) . &quot;Principal Component&quot; Analysis . Now we have our components (=eigenvectors), and we have them &quot;ranked&quot; by their &quot;significance.&quot; Next we will eliminate one or more of the less-significant directions of variance. In other words, we will project the data onto the various principal components by projecting along the less-significant components. Or even simpler: We will &quot;squish&quot; the data along the smallest-variance directions. . For the above 3D dataset, we&#39;re going to squish it into a 2D pancake -- by projecting along the direction of the 3rd (purple) eigenvector onto the plane defined by the 1st (red) and 2nd (greenish) eigenvectors. . Yea, but how to do this projection? . Projecting the data . It&#39;s actually not that big of a deal. All we have to do is multiply by the eigenvector (matrix)! . OH WAIT! Hey, you want to see a cool trick!? Check this out: . lambdas, vs = sorted_eig(cov) proj_cov = vs.T @ cov @ vs # project the covariance matrix, using eigenvectors proj_cov . array([[ 1.07311435e+00, 4.47193943e-18, -4.45219967e-17], [ 5.25950058e-17, 2.67240036e-01, 6.70099547e-17], [-7.05170575e-17, 4.02835192e-17, 2.90836287e-02]]) . What was THAT? Let me clean that up a bit for you... . proj_cov[np.abs(proj_cov) &lt; 1e-15] = 0 proj_cov . array([[1.07311435, 0. , 0. ], [0. , 0.26724004, 0. ], [0. , 0. , 0.02908363]]) . Important point: What you just saw is the whole reason eigenvectors get used for so many things, because they give you a &#39;coordinate system&#39; where different &#39;directions&#39; decouple from each other. See, the system has its own (German: eigen) inherent set of orientations which are different the &#39;arbitrary&#39; coordinates that we &#39;humans&#39; may have assigned initially. . The numbers in that matrix are the covariances in the directions of the eigenvectors, instead of in the directions of the original x, y, and z. . So really all we have to do is make a coordinate transformation using the matrix of eigenvectors, and then in order to project we&#39;ll literally just drop a whole index&#39;s-worth of data-dimension in this new coordinate system. :-) . So, instead of $x$, $y$ and $z$, let&#39;s have three coordinates which (following physicist-notation) we&#39;ll call $q_1$, $q_2$ and $q_3$, and these will run along the directions given by the three eigenvectors. . Let&#39;s write our data as a N-by-3 matrix, where N is the number of data points we have. . data = np.stack((x,y,z),axis=1) data.shape # we had a 100 data points, so expecting 100x3 matrix . (100, 3) . There are two ways of doing this, and I&#39;ll show you that they&#39;re numerically equivalent: . Use all the eigenvectors to &quot;rotate&quot; the full dataset into the new coordinate system. Then perform a projection by truncating the last column of the rotated data. | Truncate the last eigenvector, which will make a 3x2 projection matrix which will project the data onto the 2D plane defined by those two eigenvectors. | Let&#39;s show them both: . print(&quot; n 1. All data, rotated into new coordinate system&quot;) W = vs[:,0:3] # keep the all the eigenvectors new_data_all = data @ W # project all the data print(&quot;Checking: new_data_all.shape =&quot;,new_data_all.shape) print(&quot;New covariance matrix = n&quot;,np.cov(new_data_all.T) ) print(&quot; n 2. Truncated data projected onto principal axes of coordinate system&quot;) W = vs[:,0:2] # keep only the first and 2nd eigenvectors print(&quot;W.shape = &quot;,W.shape) new_data_proj = data @ W # project print(&quot;Checking: new_data_proj.shape =&quot;,new_data_proj.shape) print(&quot;New covariance matrix in projected space = n&quot;,np.cov(new_data_proj.T) ) # Difference between them diff = new_data_all[:,0:2] - new_data_proj print(&quot; n Absolute maximum difference between the two methods = &quot;,np.max(np.abs(diff))) . 1. All data, rotated into new coordinate system Checking: new_data_all.shape = (100, 3) New covariance matrix = [[1.07311435e+00 7.64444687e-17 3.77081428e-17] [7.64444687e-17 2.67240036e-01 1.21906748e-16] [3.77081428e-17 1.21906748e-16 2.90836287e-02]] 2. Truncated data projected onto principal axes of coordinate system W.shape = (3, 2) Checking: new_data_proj.shape = (100, 2) New covariance matrix in projected space = [[1.07311435e+00 7.64444687e-17] [7.64444687e-17 2.67240036e-01]] Absolute maximum difference between the two methods = 0.0 . ...Nada. The 2nd method will be faster computationally though, because it doesn&#39;t calculate stuff you&#39;re going to throw away. . One more comparison between the two methods. Let&#39;s take a look at the &quot;full&quot; dataset (in blue) vs. the projected dataset (in red): . fig = go.Figure(data=[(go.Scatter3d(x=new_data_all[:,0], y=new_data_all[:,1], z=new_data_all[:,2], mode=&#39;markers&#39;, marker=dict(size=4,opacity=0.5), name=&quot;full data&quot; ))]) fig.add_trace(go.Scatter3d(x=new_data_proj[:,0], y=new_data_proj[:,1], z=new_data_proj[:,0]*0, mode=&#39;markers&#39;, marker=dict(size=4,opacity=0.5), name=&quot;projected&quot; ) ) fig.update_layout(scene_aspectmode=&#39;data&#39;) fig.show() . . . (Darn it, [if only Plot.ly would support orthographic projections](https://community.plot.ly/t/orthographic-projection-for-3d-plots/3897) [[2](https://community.plot.ly/t/orthographic-projection-instead-of-perspective-for-3d-plots/10035)] it&#39;d be a lot easier to visually compare the two datasets!) . Beyond 3D . So typically we use PCA to throw out many more dimensions than just one. Often this is used for data visualization but it&#39;s also done for feature reduction, i.e. to send less data into your machine learning algorithm. (PCA can even be used just as a &quot;multidimensional linear regression&quot; algorithm, but you wouldn&#39;t want to!) . How do you know how many dimensions to throw out? . In other words, how many &#39;components&#39; should you choose to keep when doing PCA? There are a few ways to make this judgement call -- it will involve a trade-off between accuracy and computational speed. You can make a graph of the amount of variance you get as a function of how many components you keep, and often there will be a an &#39;elbow&#39; at some point on the graph indicating a good cut-off point to choose. Stay tuned as we do the next example; we&#39;ll make such a graph. For more on this topic, see this post by Arthur Gonsales. . Example: Handwritten Digits . The scikit-learn library uses this as an example and I like it. It goes as follows: . Take a dataset of tiny 8x8 pixel images of handwritten digits. | Run PCA to break it down from 8x8=64 dimensions to just two or 3 dimensions. | Show on a plot how the different digits cluster in different regions of the space. | (This part we&#39;ll save for the Appendix: Draw boundaries between the regions and use this as a classifier.) | To be clear: In what follows, each pixel of the image counts as a &quot;feature,&quot; i.e. as a dimension. Thus an entire image can be represented as a single point in a multidimensional space, in which distance from the origin along each dimension is given by the pixel intensity. In this example, the input space is not a 2D space that is 8 units wide and 8 units long, rather it consists of 8x8= 64 dimensions. . from sklearn.datasets import load_digits from sklearn.decomposition import PCA digits = load_digits() X = digits.data / 255.0 Y = digits.target print(X.shape, Y.shape,&#39; n&#39;) # Let&#39;s look a a few examples for i in range(8): # show 8 examples print(&quot;This is supposed to be a &#39;&quot;,Y[i],&quot;&#39;:&quot;,sep=&quot;&quot;) plt.imshow(X[i].reshape([8,8])) plt.show() . (1797, 64) (1797,) This is supposed to be a &#39;0&#39;: . This is supposed to be a &#39;1&#39;: . This is supposed to be a &#39;2&#39;: . This is supposed to be a &#39;3&#39;: . This is supposed to be a &#39;4&#39;: . This is supposed to be a &#39;5&#39;: . This is supposed to be a &#39;6&#39;: . This is supposed to be a &#39;7&#39;: . Now let&#39;s do the PCA thang... First we&#39;ll try going down to 2 dimensions. This isn&#39;t going to work super great but we&#39;ll try: . digits_cov = np.cov(X.T) print(&quot;digits_cov.shape = &quot;,digits_cov.shape) lambdas, vs = sorted_eig(np.array(digits_cov)) W = vs[:,0:2] # just keep two dimensions proj_digits = X @ W print(&quot;proj_digits.shape = &quot;, proj_digits.shape) # Make the plot fig = go.Figure(data=[go.Scatter(x=proj_digits[:,0], y=proj_digits[:,1],# z=Y, #z=proj_digits[:,2], mode=&#39;markers&#39;, marker=dict(size=6, opacity=0.7, color=Y), text=[&#39;digit=&#39;+str(j) for j in Y] )]) fig.update_layout( xaxis_title=&quot;q_1&quot;, yaxis_title=&quot;q_2&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.update_layout(scene_camera=dict(up=dict(x=0, y=0, z=1), center=dict(x=0, y=0, z=0), eye=dict(x=0, y=0, z=1.5))) fig.show() . digits_cov.shape = (64, 64) proj_digits.shape = (1797, 2) . . . This is &#39;sort of ok&#39;: There are some regions that are mostly one kind of digit. But you may say there&#39;s there&#39;s too much intermingling between classes for a lot of this plot. So let&#39;s try it again with 3 dimensions for PCA: . W = vs[:,0:3] # just three dimensions proj_digits = X @ W print(&quot;proj_digits.shape = &quot;, proj_digits.shape) # Make the plot, separate them by &quot;z&quot; which is the digit of interest. fig = go.Figure(data=[go.Scatter3d(x=proj_digits[:,0], y=proj_digits[:,1], z=proj_digits[:,2], mode=&#39;markers&#39;, marker=dict(size=4, opacity=0.8, color=Y, showscale=True), text=[&#39;digit=&#39;+str(j) for j in Y] )]) fig.update_layout(title=&quot;8x8 Handwritten Digits&quot;, xaxis_title=&quot;q_1&quot;, yaxis_title=&quot;q_2&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . proj_digits.shape = (1797, 3) . . . Now we can start to see some definition! The 6&#39;s are pretty much in one area, the 2&#39;s are in another area, and the 0&#39;s are in still another, and so on. There is some intermingling to be sure (particularly between the 5&#39;s and 8&#39;s), but you can see that this &#39;kind of&#39; gets the job done, and instead of dealing with 64 dimensions, we&#39;re down to 3! . Graphing Variance vs. Components . Earlier we asked the question of how many components one should keep. To answer this quantitatively, we note that the eigenvalues of the covariance matrix are themselves measures of the variance in the datset. So these eigenvalues encode the &#39;significance&#39; that each feature-dimension has in the overall dataset. We can plot these eigenvalues in order and then look for a &#39;kink&#39; or &#39;elbow&#39; in the graph as a place to truncate our representation... . plt.plot( np.abs(lambdas)/np.sum(lambdas) ) plt.xlabel(&#39;Number of components&#39;) plt.ylabel(&#39;Significance&#39;) plt.show() . ...So, if we were wanting to represent this data in more than 3 dimensions but less than the full 64, we might choose around 10 principal compnents, as this looks like roughly where the &#39;elbow&#39; in the graph lies. . Interpretability . What is the meaning of the new coordinate axes or &#39;features&#39; $q_1$, $q_2$, etc? Sometimes there exists a compelling physical intepretation of these features (e.g., as in the case of eigenmodes of coupled oscillators), but often...there may not be any. And yet we haven&#39;t even done any &#39;real machine learning&#39; at this point! ;-) . This is an important topic. Modern data regulations such as the European Union&#39;s GDPR require that models used in algorithmic decision-making be &#39;explainable&#39;. If the data being fed into such algorithmics is already abstracted via methods such as PCA, this could be an issue. Thankfully, the linearity of the components mean that one can describe each principal component as a linear combination of inputs. . Further reading . There are many books devoted entirely to the intricacies of PCA and its applications. Hopefully this post has helped you get a better feel for how to construct a PCA transformation and what it might be good for. To expand on this see the following... . Examples &amp; links . &quot;Eigenstyle: Principal Component Analysis and Fashion&quot; by Grace Avery. Uses PCA on Fashion-MNIST. It&#39;s good! | Neat paper by my friend Dr. Ryan Bebej from when he was a student and used PCA to classify locomotion types of prehistoric acquatic mammals based on skeletal measurements alone. | Andrew Ng&#39;s Machine Learning Course, Lecture on PCA. How I first learned about this stuff. | PCA using Python by Michael Galarnyk. Does similar things to what I&#39;ve done here, although maybe better! | Plot.ly PCA notebook examples | . Appendix A: Overkill: Bigger Handwritten Digits . Sure, 8x8 digit images are boring. What about 28x28 images, as in the MNIST dataset? Let&#39;s roll... . #from sklearn.datasets import fetch_mldata from sklearn.datasets import fetch_openml from random import sample #mnist = fetch_mldata(&#39;MNIST original&#39;) mnist = fetch_openml(&#39;mnist_784&#39;, version=1, cache=True) X2 = mnist.data / 255 Y2 = np.array(mnist.target,dtype=np.int) #Let&#39;s grab some indices for random suffling indices = list(range(X2.shape[0])) # Let&#39;s look a a few examples for i in range(8): # 8 is good i = sample(indices,1) print(&quot;This is supposed to be a &quot;,Y2[i][0],&quot;:&quot;,sep=&quot;&quot;) plt.imshow(X2[i].reshape([28,28])) plt.show() . This is supposed to be a 1: . This is supposed to be a 9: . This is supposed to be a 8: . This is supposed to be a 7: . This is supposed to be a 3: . This is supposed to be a 5: . This is supposed to be a 2: . This is supposed to be a 6: . # Like we did before... Almost the whole PCA method is the next 3 lines! mnist_cov = np.cov(X2.T) lambdas, vs = sorted_eig(np.array(mnist_cov)) W = vs[:,0:3] # Grab the 3 most significant dimensions # Plotting all 70,000 data points is going to be too dense too look at. # Instead let&#39;s grab a random sample of 5,000 points n_plot = 5000 indices = sample(list(range(X2.shape[0])), n_plot) proj_mnist = np.array(X2[indices] @ W, dtype=np.float32) # Last step of PCA: project fig = go.Figure(data=[go.Scatter3d(x=proj_mnist[:,0], y=proj_mnist[:,1], z=proj_mnist[:,2], mode=&#39;markers&#39;, marker=dict(size=4, opacity=0.7, color=Y2[indices], showscale=True), text=[&#39;digit=&#39;+str(j) for j in Y2[indices]] )]) fig.show() . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: ComplexWarning: Casting complex values to real discards the imaginary part . . . ...bit of a mess. Not as cleanly separated as the 8x8 image examples. You can see that the 0&#39;s are well separated from the 1&#39;s and the 3&#39;s, but everything else is pretty mixed together. (I suspect the 1&#39;s are clustered strongly because they involve the most dark pixels.) . If you wanted to push this further then either keeping more dimensions (thereby making it un-visualizable) or just using a different method entirely (e.g. t-SNE or even better: UMAP) would be the way to go. Still, it&#39;s neat to see that you can get somewhat intelligible results in 3D even on this &#39;much harder&#39; problem. . Appendix B: Because We Can: Turning it into a Classifier . ...But let&#39;s not do a neural network because all I ever do are neural networks, and because I don&#39;t want to have take the time &amp; space to explain how they work or load in external libraries. Let&#39;s do k-nearest-neighbors instead, because it&#39;s intuitively easy to grasp and it&#39;s not hard to code up: . For any new point we want to evaluate, we take a &#39;vote&#39; of whatever some number (called $k$) of its nearest neighbor points are already assigned as, and we set the class of the new point according to that vote. . Making a efficient classifier is all about finding the *boundaries* between regions (and usually subject to some user-adjustable parameter like $k$ or some numerical threshold). Finding these boundaries can be about finding the &#39;edge cases&#39; that cause the system to &#39;flip&#39; (discontinuously) from one result to another. However, we are *not* going to make an efficient classifier today. ;-) . Let&#39;s go back to the 8x8 digits example, and split it into a training set and a testing set (so we can check ourselves): . # random shuffled ordering of the whole thing indices = sample(list(range(X.shape[0])), X.shape[0]) X_shuf, Y_shuf = X[indices,:], Y[indices] # 80-20 train-test split max_train_ind = int(0.8*X.shape[0]) X_train, Y_train = X_shuf[0:max_train_ind,:], Y_shuf[0:max_train_ind] X_test, Y_test = X_shuf[max_train_ind:-1,:], Y_shuf[max_train_ind:-1] # Do PCA on training set train_cov = np.cov(X_train.T) ell, v = sorted_eig(np.array(train_cov)) pca_dims = 3 # number of top &#39;dimensions&#39; to take W_train = v[:,0:pca_dims] proj_train = X_train @ W_train # also project the testing set while we&#39;re at it proj_test = X_test @ W_train # yes, same W_train . Now let&#39;s make a little k-nearest neighbors routine... . from collections import Counter def knn_predict(xnew, proj_train, Y_train, k=3): &quot;&quot;&quot; xnew is a new data point that has the same shape as one row of proj_train. Given xnew, calculate the (squared) distance to all the points in X_train to find out which ones are nearest. &quot;&quot;&quot; distances = ((proj_train - xnew)**2).sum(axis=1) # stick on an extra column of indexing &#39;hash&#39; for later use after we sort dists_i = np.stack( (distances, np.array(range(Y_train.shape[0]) )),axis=1 ) dists_i = dists_i[dists_i[:,0].argsort()] # sort in ascending order of distance knn_inds = (dists_i[0:k,-1]).astype(np.int) # Grab the indexes for k nearest neighbors # take &#39;votes&#39;: knn_targets = list(Y_train[knn_inds]) # which classes the nn&#39;s belong to votes = Counter(knn_targets) # count up how many of each class are represented return votes.most_common(1)[0][0] # pick the winner, or the first member of a tie # Let&#39;s test it on the first element of the testing set x, y_true = proj_test[0], Y_test[0] guess = knn_predict(x, proj_train, Y_train) print(&quot;guess = &quot;,guess,&quot;, true = &quot;,y_true) . guess = 6 , true = 6 . Now let&#39;s try it for the &#39;unseen&#39; data in the testing set, and see how we do... . mistakes, n_test = 0, Y_test.shape[0] for i in range(n_test): x = proj_test[i] y_pred = knn_predict(x, proj_train, Y_train, k=3) y_true = Y_test[i] if y_true != y_pred: mistakes += 1 if i &lt; 20: # show some examples print(&quot;x, y_pred, y_true =&quot;,x, y_pred, y_true, &quot;YAY!&quot; if y_pred==y_true else &quot; BOO. :-(&quot;) print(&quot;...skipping a lot...&quot;) print(&quot;Total Accuracy =&quot;, (n_test-mistakes)/n_test*100,&quot;%&quot;) . x, y_pred, y_true = [ 0.06075339 0.00153272 -0.0477644 ] 6 6 YAY! x, y_pred, y_true = [ 0.04083212 0.09757529 -0.05361896] 1 1 YAY! x, y_pred, y_true = [-0.0199586 -0.00778773 0.00972962] 8 5 BOO. :-( x, y_pred, y_true = [ 0.02400112 -0.07267613 0.02774141] 0 0 YAY! x, y_pred, y_true = [0.01180771 0.03483923 0.07526469] 1 7 BOO. :-( x, y_pred, y_true = [ 0.00379226 -0.06269449 -0.00195609] 0 0 YAY! x, y_pred, y_true = [-0.06832135 -0.05396545 0.02980845] 9 9 YAY! x, y_pred, y_true = [-0.02397417 -0.04914796 0.0109273 ] 5 5 YAY! x, y_pred, y_true = [ 0.08213707 -0.01608953 -0.08072889] 6 6 YAY! x, y_pred, y_true = [ 0.03056858 -0.04852946 0.02204423] 0 0 YAY! x, y_pred, y_true = [-0.02124777 0.03623541 -0.01773196] 8 8 YAY! x, y_pred, y_true = [0.03035896 0.01398381 0.01415554] 8 8 YAY! x, y_pred, y_true = [ 0.0214849 0.02114674 -0.08951798] 1 1 YAY! x, y_pred, y_true = [ 0.07878152 0.03312015 -0.06488347] 6 6 YAY! x, y_pred, y_true = [-0.01294308 0.00158962 -0.01255491] 8 5 BOO. :-( x, y_pred, y_true = [ 0.01351581 0.11000321 -0.03895516] 1 1 YAY! x, y_pred, y_true = [0.0081306 0.01683952 0.05911389] 7 1 BOO. :-( x, y_pred, y_true = [0.06497268 0.02817075 0.07118004] 4 4 YAY! x, y_pred, y_true = [-0.03879657 -0.04460611 0.02833793] 9 5 BOO. :-( x, y_pred, y_true = [-0.05975051 0.03713843 -0.07174727] 2 2 YAY! ...skipping a lot... Total Accuracy = 76.88022284122563 % . ...eh! Not bad, not amazing. You can improve the accuracy if you go back up and increase the number of PCA dimensions beyond 3, and/or increase the value of $k$. Go ahead and try it! . (For 10 dimensions and $k=7$, I got 97.7% accuracy. The highest I ever got it was 99%, but that was really working overtime, computationally speaking; the point of PCA is to let you dramatically reduce your workload while retaining reasonably high accuracy.) . Just to reiterate: This is NOT supposed to be a state of the art classifier! It&#39;s just a toy that does pretty well and helps illustrate PCA without being hard to understand or code. . The End . Thanks for sticking around! Hope this was interesting. PCA is pretty simple, and yet really useful! ...and writing this really helped me to better understand it. ;-) .",
            "url": "https://j-k-projects.github.io/boxofpebbles/2020/04/15/PCA-From-Scratch.html",
            "relUrl": "/2020/04/15/PCA-From-Scratch.html",
            "date": " • Apr 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://j-k-projects.github.io/boxofpebbles/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://j-k-projects.github.io/boxofpebbles/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://j-k-projects.github.io/boxofpebbles/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://j-k-projects.github.io/boxofpebbles/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}